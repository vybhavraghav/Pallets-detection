{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After deleting references:\n",
      "Allocated memory: 0.00 MB\n",
      "Cached memory: 0.00 MB\n",
      "\n",
      "After emptying cache:\n",
      "Allocated memory: 0.00 MB\n",
      "Cached memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Helper function to check GPU memory usage\n",
    "def print_gpu_memory():\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "\n",
    "print(\"\\nAfter deleting references:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# Step 3: Free up GPU memory manually using torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAfter emptying cache:\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated after clearing cache: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Manually invoke garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Memory allocated after clearing cache: {torch.cuda.memory_allocated()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from segment_anything import SamPredictor, sam_model_registry\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the model in half precision (mixed precision) to save memory\n",
    "# def load_sam_model(model_name='vit_h', checkpoint_path='weights/sam_vit_h_4b8939.pth'):\n",
    "#     # Register model (use correct variant based on your hardware capabilities)\n",
    "#     sam = sam_model_registry[model_name](checkpoint=checkpoint_path).half().cuda()\n",
    "#     predictor = SamPredictor(sam)\n",
    "#     return predictor\n",
    "\n",
    "# # Run SAM on an image with memory optimization\n",
    "# def run_sam_inference(predictor, image_path):\n",
    "#     # Load and preprocess the image\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image = np.array(image)\n",
    "    \n",
    "#     # Move the image to predictor and apply half precision where possible\n",
    "#     predictor.set_image(image)\n",
    "    \n",
    "#     # Define points for segmentation (this can be customized)\n",
    "#     input_points = np.array([[image.shape[1] // 2, image.shape[0] // 2]])  # Center point as example\n",
    "#     input_labels = np.array([1])  # Label indicating foreground\n",
    "    \n",
    "#     with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "#         # Run SAM prediction\n",
    "#         masks, scores, _ = predictor.predict(\n",
    "#             point_coords=input_points,\n",
    "#             point_labels=input_labels,\n",
    "#             multimask_output=False  # Set to False if single mask is sufficient to save memory\n",
    "#         )\n",
    "    \n",
    "#     # Clear memory explicitly after inference\n",
    "#     torch.cuda.empty_cache()\n",
    "#     return masks, scores\n",
    "\n",
    "# # Usage example\n",
    "# model_name = 'vit_h'  # or other variants of SAM like 'vit_b' or 'vit_l' for smaller sizes\n",
    "# checkpoint_path = 'weights/sam_vit_h_4b8939.pth'  # Path to SAM checkpoint file\n",
    "# image_path = 'Pallets/1000736-6303_jpg.rf.92c0d6c8403755071eac22527c9ad815.jpg'  # Path to the input image\n",
    "\n",
    "# # Load the model and perform inference\n",
    "# predictor = load_sam_model(model_name, checkpoint_path)\n",
    "# masks, scores = run_sam_inference(predictor, image_path)\n",
    "\n",
    "# # Display result (optional, based on the framework you're using)\n",
    "# print(\"Segmentation Masks:\", masks)\n",
    "# print(\"Confidence Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "\n",
    "# def visualize_segmentation(image_path, mask):\n",
    "#     # Load the original image\n",
    "#     image = cv2.imread(image_path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV) to RGB for matplotlib\n",
    "\n",
    "#     # Resize mask to match the image shape if necessary\n",
    "#     if mask.shape[:2] != image.shape[:2]:\n",
    "#         mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
    "\n",
    "#     # Create an RGBA image with the mask as an alpha channel\n",
    "#     mask_color = np.zeros_like(image, dtype=np.uint8)\n",
    "#     mask_color[mask > 0] = [255, 0, 0]  # Set color for the mask, e.g., red\n",
    "    \n",
    "#     # Blend the original image with the mask overlay\n",
    "#     alpha = 0.5  # Transparency factor\n",
    "#     overlay = cv2.addWeighted(image, 1 - alpha, mask_color, alpha, 0)\n",
    "\n",
    "#     # Plot the original image with the segmentation overlay\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.imshow(overlay)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(\"Segmentation Visualization\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage example\n",
    "# image_path = 'Pallets/1000736-6303_jpg.rf.92c0d6c8403755071eac22527c9ad815.jpg'\n",
    "# masks, _ = run_sam_inference(predictor, image_path)  # Assume this returns a binary mask\n",
    "# visualize_segmentation(image_path, masks[0])  # Use the first mask if multiple are returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "# import numpy as np\n",
    "# import gc\n",
    "\n",
    "# def show_anns(anns):\n",
    "#     if len(anns) == 0:\n",
    "#         return\n",
    "#     sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_autoscale_on(False)\n",
    "#     polygons = []\n",
    "#     color = []\n",
    "#     for ann in sorted_anns:\n",
    "#         m = ann['segmentation']\n",
    "#         img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "#         color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "#         for i in range(3):\n",
    "#             img[:,:,i] = color_mask[i]\n",
    "#         ax.imshow(np.dstack((img, m*0.35)))\n",
    "\n",
    "\n",
    "# sam = sam_model_registry[\"default\"](checkpoint=\"weights/sam_vit_h_4b8939.pth\").half().cuda()\n",
    "# mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "# image = cv2.imread('Pallets/1000736-6303_jpg.rf.92c0d6c8403755071eac22527c9ad815.jpg')\n",
    "# image = cv2.resize(image, (208,208))\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# sam.to(device='cuda')\n",
    "# masks = mask_generator.generate(image)\n",
    "# print(len(masks))\n",
    "# print(masks[0].keys())\n",
    "# plt.figure(figsize=(20,20))\n",
    "# plt.imshow(image)\n",
    "# show_anns(masks)\n",
    "# plt.axis('off')\n",
    "# plt.show() \n",
    "# del(masks)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the SAM model and initialize the SAMMaskGenerator\n",
    "def load_sam_mask_generator(model_name='vit_h', checkpoint_path='sam_vit_h_4b8939.pth'):\n",
    "    sam_model = sam_model_registry[model_name](checkpoint=checkpoint_path).half().cuda()\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam_model)\n",
    "    return mask_generator\n",
    "\n",
    "# Generate masks for an entire image\n",
    "def generate_masks(mask_generator, image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.array(image)\n",
    "    print(image.dtype)\n",
    "    \n",
    "    # Run SAM Mask Generator\n",
    "    with torch.no_grad():  # Disable gradient computation to save memory\n",
    "        masks = mask_generator.generate(image)\n",
    "    \n",
    "    return masks\n",
    "\n",
    "# Visualize the generated masks\n",
    "def visualize_masks(image_path, masks):\n",
    "    # Load the original image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = np.array(image, dtype=np.float16)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a blank canvas for the mask overlay\n",
    "    mask_overlay = np.zeros_like(image, dtype=np.uint8)\n",
    "\n",
    "    # Apply each mask with a random color for visualization\n",
    "    for mask in masks:\n",
    "        color = np.random.randint(0, 255, size=3, dtype=np.uint8)\n",
    "        mask_overlay[mask['segmentation']] = color\n",
    "\n",
    "    # Blend the original image with the mask overlay\n",
    "    alpha = 0.5  # Transparency factor\n",
    "    overlay = cv2.addWeighted(image, 1 - alpha, mask_overlay, alpha, 0)\n",
    "\n",
    "    # Display the result\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Automatic Mask Generation with SAM\")\n",
    "    plt.show()\n",
    "\n",
    "# Usage example\n",
    "model_name = 'vit_h'\n",
    "checkpoint_path = 'weights/sam_vit_h_4b8939.pth'\n",
    "image_path = 'Pallets/1000736-6303_jpg.rf.92c0d6c8403755071eac22527c9ad815.jpg'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize SAMMaskGenerator and generate masks\n",
    "mask_generator = load_sam_mask_generator(model_name, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Visualize the segmentation masks\u001b[39;00m\n\u001b[1;32m      4\u001b[0m visualize_masks(image_path, masks)\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mgenerate_masks\u001b[0;34m(mask_generator, image_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Run SAM Mask Generator\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient computation to save memory\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[43mmask_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masks\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:245\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_per_batch, points_for_image):\n\u001b[0;32m--> 245\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(batch_data)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:279\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    277\u001b[0m in_points \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(transformed_points, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    278\u001b[0m in_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(in_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint, device\u001b[38;5;241m=\u001b[39min_points\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 279\u001b[0m masks, iou_preds, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Serialize predictions and store in MaskData\u001b[39;00m\n\u001b[1;32m    287\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData(\n\u001b[1;32m    288\u001b[0m     masks\u001b[38;5;241m=\u001b[39mmasks\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    289\u001b[0m     iou_preds\u001b[38;5;241m=\u001b[39miou_preds\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    290\u001b[0m     points\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mas_tensor(points\u001b[38;5;241m.\u001b[39mrepeat(masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/predictor.py:222\u001b[0m, in \u001b[0;36mSamPredictor.predict_torch\u001b[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    219\u001b[0m     points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Embed prompts\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Predict masks\u001b[39;00m\n\u001b[1;32m    229\u001b[0m low_res_masks, iou_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmask_decoder(\n\u001b[1;32m    230\u001b[0m     image_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,\n\u001b[1;32m    231\u001b[0m     image_pe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprompt_encoder\u001b[38;5;241m.\u001b[39mget_dense_pe(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     multimask_output\u001b[38;5;241m=\u001b[39mmultimask_output,\n\u001b[1;32m    235\u001b[0m )\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/modeling/prompt_encoder.py:155\u001b[0m, in \u001b[0;36mPromptEncoder.forward\u001b[0;34m(self, points, boxes, masks)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     coords, labels \u001b[38;5;241m=\u001b[39m points\n\u001b[0;32m--> 155\u001b[0m     point_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     sparse_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sparse_embeddings, point_embeddings], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/modeling/prompt_encoder.py:86\u001b[0m, in \u001b[0;36mPromptEncoder._embed_points\u001b[0;34m(self, points, labels, pad)\u001b[0m\n\u001b[1;32m     84\u001b[0m     points \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([points, padding_point], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([labels, padding_label], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m point_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_image_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m point_embedding[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     88\u001b[0m point_embedding[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_a_point_embed\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/modeling/prompt_encoder.py:214\u001b[0m, in \u001b[0;36mPositionEmbeddingRandom.forward_with_coords\u001b[0;34m(self, coords_input, image_size)\u001b[0m\n\u001b[1;32m    212\u001b[0m coords[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m coords[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m image_size[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    213\u001b[0m coords[:, :, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m coords[:, :, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m image_size[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pe_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/peer_ws/peer/lib/python3.10/site-packages/segment_anything/modeling/prompt_encoder.py:189\u001b[0m, in \u001b[0;36mPositionEmbeddingRandom._pe_encoding\u001b[0;34m(self, coords)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\u001b[39;00m\n\u001b[1;32m    188\u001b[0m coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m coords \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 189\u001b[0m coords \u001b[38;5;241m=\u001b[39m \u001b[43mcoords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding_gaussian_matrix\u001b[49m\n\u001b[1;32m    190\u001b[0m coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m coords\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# outputs d_1 x ... x d_n x C shape\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: float != c10::Half"
     ]
    }
   ],
   "source": [
    "masks = generate_masks(mask_generator, image_path)\n",
    "\n",
    "# Visualize the segmentation masks\n",
    "visualize_masks(image_path, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
